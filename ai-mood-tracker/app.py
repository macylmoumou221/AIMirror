"""Streamlit entry point for AI Mood Tracker - Cloud Version with Browser Camera Access."""

from __future__ import annotations

from datetime import datetime
from pathlib import Path
from typing import Optional
from PIL import Image
import io

import cv2
import numpy as np
import streamlit as st

from emotion_detector import (
	EmotionAnalyzer,
	EmotionResult,
	append_log_entry,
	ensure_log_file,
	get_daily_summary,
	get_recent_entries,
	save_distribution_chart,
	_HAS_DEEPFACE,
	_HAS_FER,
	_DEEPFACE_ERROR,
	_FER_ERROR,
)


BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"
VISUALS_DIR = BASE_DIR / "visuals"
LOG_PATH = DATA_DIR / "mood_log.csv"
CHART_PATH = VISUALS_DIR / "daily_mood_chart.png"


st.set_page_config(
	page_title="AI Mood Tracker",
	page_icon="ï¿½",
	layout="wide",
)


@st.cache_resource
def get_analyzer() -> EmotionAnalyzer:
	"""Load and cache the emotion analyzer."""
	try:
		return EmotionAnalyzer()
	except ImportError as exc:
		st.error("No emotion detection backend available.")
		st.error(str(exc))
		st.info("Try installing: `pip install deepface fer`")
		st.stop()


def analyze_image(image_data, analyzer: EmotionAnalyzer) -> Optional[EmotionResult]:
	"""Analyze emotion from uploaded/captured image."""
	try:
		# Convert PIL Image to numpy array
		if isinstance(image_data, Image.Image):
			img = np.array(image_data)
		else:
			img = np.array(Image.open(io.BytesIO(image_data.read())))
		
		# Convert RGB to BGR for OpenCV
		if len(img.shape) == 3 and img.shape[2] == 3:
			img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
		else:
			img_bgr = img
		
		# Analyze emotion
		result = analyzer.analyze(img_bgr)
		return result
	except Exception as e:
		st.error(f"Error analyzing image: {e}")
		return None


def draw_emotion_overlay(image: Image.Image, result: EmotionResult) -> Image.Image:
	"""Draw emotion label on image."""
	img_array = np.array(image)
	
	# Convert RGB to BGR for OpenCV
	img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
	
	# Draw emotion text
	text = f"{result.dominant_emotion}: {result.confidence:.1f}%"
	cv2.putText(
		img_bgr,
		text,
		(10, 30),
		cv2.FONT_HERSHEY_SIMPLEX,
		1.0,
		(0, 255, 0),
		2,
	)
	
	# Convert back to RGB
	img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
	return Image.fromarray(img_rgb)


def main() -> None:
	ensure_log_file(LOG_PATH)

	st.title("AI Mood Tracker")
	st.markdown("*Detect and track your emotions in real-time using AI*")

	# Display backend availability info
	with st.sidebar:
		st.markdown("### Backend Status")
		if _HAS_DEEPFACE:
			st.success("DeepFace available")
		else:
			st.warning("DeepFace unavailable")
			if _DEEPFACE_ERROR:
				with st.expander("Error details"):
					st.code(_DEEPFACE_ERROR[:200])
		
		if _HAS_FER:
			st.success("FER available")
		else:
			st.warning("FER unavailable")
			if _FER_ERROR:
				with st.expander("Error details"):
					st.code(_FER_ERROR[:200])
		
		if not _HAS_DEEPFACE and not _HAS_FER:
			st.error("No emotion backend available!")
			st.stop()

	analyzer = get_analyzer()

	# Main layout
	col1, col2 = st.columns([2, 1])

	with col1:
		st.subheader("Camera Input")
		
		# Camera input - this will ask for camera permission in browser
		camera_photo = st.camera_input("Take a photo to analyze your emotion")
		
		if camera_photo is not None:
			# Analyze the captured photo
			with st.spinner("Analyzing emotion..."):
				result = analyze_image(camera_photo, analyzer)
			
			if result:
				# Show image with emotion overlay
				img = Image.open(camera_photo)
				img_with_overlay = draw_emotion_overlay(img, result)
				st.image(img_with_overlay, caption=f"Detected: {result.dominant_emotion} ({result.confidence:.1f}%)", use_container_width=True)
				
				# Log the result
				append_log_entry(LOG_PATH, result, source="browser_camera")
				
				st.success(f"Logged: **{result.dominant_emotion}** ({result.confidence:.1f}%)")
			else:
				st.warning("No face detected. Please try again with better lighting.")

	with col2:
		st.subheader("Today's Summary")
		
		summary = get_daily_summary(LOG_PATH)
		
		if summary and summary.total_scans > 0:
			st.metric("Total Detections", summary.total_scans)
			st.metric("Dominant Emotion", summary.dominant_emotion)
			
			# Show emotion distribution
			st.markdown("**Emotion Distribution:**")
			for emotion, pct in summary.distribution.items():
				st.progress(pct, text=f"{emotion}: {pct*100:.1f}%")
			
			# Generate and show chart
			save_distribution_chart(summary.distribution, CHART_PATH, "Daily Mood Distribution")
			if CHART_PATH.exists():
				st.image(str(CHART_PATH), caption="Daily Mood Distribution", use_container_width=True)
		else:
			st.info("No data logged today. Take a photo to get started!")

	# Recent entries
	st.subheader("Recent Entries")
	recent = get_recent_entries(LOG_PATH, limit=10)
	
	if not recent.empty:
		# Format the dataframe
		recent_display = recent.copy()
		recent_display['timestamp'] = recent_display['timestamp'].dt.strftime('%H:%M:%S')
		recent_display['confidence'] = recent_display['confidence'].apply(lambda x: f"{x:.1f}%")
		st.dataframe(recent_display, use_container_width=True, hide_index=True)
	else:
		st.info("No entries yet. Start by taking a photo!")

	# Instructions
	with st.expander("How to use"):
		st.markdown("""
		1. **Allow Camera Access**: Click the camera button above and allow browser access to your camera
		2. **Take a Photo**: Click the capture button to take a photo
		3. **View Results**: Your emotion will be detected and logged automatically
		4. **Track Progress**: Check the summary and recent entries to see your mood patterns
		
		**Note**: This works entirely in your browser - no video is stored, only emotion data is logged.
		""")


if __name__ == "__main__":
	main()
